{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T12:14:36.768023",
     "start_time": "2017-01-21T12:14:36.765754"
    }
   },
   "source": [
    "# Dask Leaflet Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T17:33:16.199652",
     "start_time": "2017-01-22T17:33:16.176020"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import os, time, sys, datetime\n",
    "import sklearn.metrics.pairwise\n",
    "import scipy.spatial.distance\n",
    "from scipy.spatial.distance import cdist\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.multiprocessing\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.diagnostics import ResourceProfiler\n",
    "from dask.dot import dot_graph\n",
    "from dask.array.core import map_blocks\n",
    "import dask.bag as db\n",
    "import dask.dataframe as df\n",
    "import random\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.CRITICAL)\n",
    "from chest import Chest\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "#from multiprocessing.pool import ThreadPool\n",
    "RESULT_DIR=\"results\"\n",
    "RESULT_FILE_PREFIX=\"pair-distance-\"\n",
    "HEADER_CSV=\"Scenario, Type, Time\"\n",
    "#BASE_DIRECTORY=os.getcwd()\n",
    "# Dask has issues with NFS home directory on Comet\n",
    "# BASE_DIRECTORY='/scratch/luckow/7146882'\n",
    "BASE_DIRECTORY='/oasis/scratch/comet/luckow/temp_project'\n",
    "#BASE_DIRECTORY='/scratch/luckow/7218009/'\n",
    "OUT_DIR=os.path.join(BASE_DIRECTORY, \"npy_stack\")\n",
    "\n",
    "FILENAMES=[\"../132k_dataset/atom_pos_132K.npy\", \"../145K_dataset/atom_pos_145K.npy\", \n",
    "          \"../300K_dataset/atom_pos_291K.npy\", '../840K_dataset/atom_pos_839K.npy']\n",
    "\n",
    "scenario = FILENAMES[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T11:01:31.928455",
     "start_time": "2017-01-21T11:01:31.925593"
    }
   },
   "source": [
    "## Dense Distance Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T15:56:25.010973",
     "start_time": "2017-01-22T15:56:24.994642"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.11 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"Make sure point_array points to correct dataset\"\"\"\n",
    "global point_array\n",
    "global cutoff\n",
    "\n",
    "cutoff=15.0\n",
    "\n",
    "\n",
    "def map_block_distance(block,  block_id=None):\n",
    "    isCompute = block_id[1]>=block_id[0] and block.shape != (1,1)# Bug ? \n",
    "                                         # Dask returns one block with shape (1,1) ID:(1, 1) Shape: (1, 1) Predicate: True Content: [[ 1.]]\n",
    "    block_length = block.shape[0]\n",
    "    logging.debug(\"ID:\" + str(block_id) + \" Shape: \" + str(block.shape) + \\\n",
    "          \" Predicate: \" + str(isCompute) + \" Content: \" + str(block) + \"\\n\")\n",
    "    if isCompute:\n",
    "        source_start = block_id[0]*block_length\n",
    "        source_end = (block_id[0]+1)*block_length\n",
    "        source_points = point_array[source_start:source_end]        \n",
    "        dest_start = block_id[1]*block_length\n",
    "        dest_end = (block_id[1]+1)*block_length\n",
    "        dest_points = point_array[dest_start:dest_end]  \n",
    "        logging.debug(\"Source Idx: %d - %d Dest. Idx: %d - %d\\n\"%(source_start, source_end, dest_start, dest_end))\n",
    "        #print \"Source Points: \" + str(source_points.compute())\n",
    "        #print \"Destination Points: \" + str(dest_points.compute())\n",
    "        distance = cdist(source_points, dest_points)\n",
    "        distances_bool = (distance < cutoff) & (distance > 0)\n",
    "        return distances_bool\n",
    "    else:\n",
    "        return np.zeros(block.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T09:39:24.821442",
     "start_time": "2017-01-16T09:39:24.819086"
    }
   },
   "source": [
    "### Testing Pairwise Distance using Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T15:56:25.017919",
     "start_time": "2017-01-22T15:56:25.012313"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "number_points = 4\n",
    "points_np = np.arange(number_points*3).reshape(number_points,3)\n",
    "point_array = da.from_array(points_np, chunks=(2, 3))\n",
    "dist_matrix = da.zeros((number_points,number_points), chunks=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T08:37:13.855321",
     "start_time": "2017-01-22T08:37:13.848990"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "point_array.numblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T08:37:14.278392",
     "start_time": "2017-01-22T08:37:14.265003"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "point_array.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T08:37:14.723072",
     "start_time": "2017-01-22T08:37:14.719120"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_matrix.numblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T08:37:15.143140",
     "start_time": "2017-01-22T08:37:15.138482"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_matrix.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T08:37:15.577175",
     "start_time": "2017-01-22T08:37:15.570447"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_matrix.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T08:37:16.175625",
     "start_time": "2017-01-22T08:37:16.168880"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_res=dist_matrix.map_blocks(map_block_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T08:37:16.809200",
     "start_time": "2017-01-22T08:37:16.773755"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances = dist_res.compute()\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-D Version with Sparse Output\n",
    "\n",
    "No persistence between NetworkX Connected Components and Pairwise Distance Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T15:15:52.097185",
     "start_time": "2017-01-22T15:15:52.091820"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2, 2), (4,))\n"
     ]
    }
   ],
   "source": [
    "number_points = 4\n",
    "points_np = np.arange(number_points*3).reshape(number_points,3)\n",
    "point_array = da.from_array(points_np, chunks=(2, 3))\n",
    "dist_matrix = da.zeros((number_points,number_points), chunks=(2,4)) # in 2. dimension only 1 chunk\n",
    "print dist_matrix.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T15:56:25.051742",
     "start_time": "2017-01-22T15:56:25.019320"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Make sure point_array points to correct dataset\"\"\"\n",
    "global point_array\n",
    "global cutoff\n",
    "\n",
    "cutoff=15.0\n",
    "\n",
    "def map_blocks_1d_sparse(block, block_id):\n",
    "    #new_block = block[:, :, None]\n",
    "    \n",
    "    isCompute = block_id[1]>=block_id[0] and block.shape != (1,1)# Bug ? \n",
    "                                         # Dask returns one block with shape (1,1) ID:(1, 1) Shape: (1, 1) Predicate: True Content: [[ 1.]]\n",
    "    block_length = block.shape[0]\n",
    "    logging.debug(\"ID:\" + str(block_id) + \" Shape: \" + str(block.shape) + \\\n",
    "          \" Predicate: \" + str(isCompute) + \" Content: \" + str(block) + \"\\n\")\n",
    "    \n",
    "    source_start = block_id[0]*block_length\n",
    "    source_end = (block_id[0]+1)*block_length\n",
    "    source_points = point_array[source_start:source_end]        \n",
    "    dest_start = 0\n",
    "    dest_end = block.shape[1]\n",
    "    dest_points = point_array[dest_start:dest_end]  \n",
    "    logging.debug(\"Source Idx: %d - %d Dest. Idx: %d - %d\\n\"%(source_start, source_end, dest_start, dest_end))\n",
    "    logging.debug(\"Source Points: \" + str(source_points.compute()))\n",
    "    logging.debug(\"Destination Points: \" + str(dest_points.compute()))\n",
    "    #if isCompute:\n",
    "    distances = cdist(source_points, dest_points) \n",
    "    #distances_bool = (distances < cutoff) & (distances > 0)\n",
    "    #logging.debug(str(distances_bool))\n",
    "    #xx, yy = np.meshgrid(np.arange(source_start, source_end), np.arange(dest_start, dest_end))\n",
    "    #logging.debug(\"xx: \" + str(xx))\n",
    "    #logging.debug(\"yy: \" + str(yy))\n",
    "    #res=np.array([zip(y,x, z) for x,y,z in zip(xx, yy, distances_bool.T)])\n",
    "    #true_res = np.array(np.where((distances < cutoff) & (distances > 0)))\n",
    "    true_res = np.array(np.where(distances < cutoff))\n",
    "    logging.debug(\"True Source: %s, Source_start: %d\"%(str(true_res[0]), source_start))\n",
    "    true_res[0] = true_res[0] + source_start # source offset for block\n",
    "    logging.debug(\"True Source Adjusted\" + str(true_res[0]))\n",
    "    true_res[1] = true_res[1] + dest_start # dest offset for block\n",
    "    res=np.array(zip(true_res[0], true_res[1]))\n",
    "    res=res[res[:,0]<res[:,1], :] # filter duplicate edges (only edges where ind1<ind2)\n",
    "    #number_pairs = block.shape[0]*block.shape[1]\n",
    "    #logging.debug(\"Result Shape: %s Block Shape: %s, Number Pairs: %d\"%(str(res.shape), str(block.shape), number_pairs))\n",
    "    #res=res.reshape(number_pairs, 3)\n",
    "    logging.debug(\"Result: \" + str(res))\n",
    "    return res\n",
    "    #else:\n",
    "    #    return np.zeros((1,2))\n",
    "    #return new_block + cdist(source_points, dest_points)<cutoff\n",
    "    #return np.array(block)\n",
    "    #return new_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T15:15:53.406766",
     "start_time": "2017-01-22T15:15:53.368241"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 2],\n",
       "       [1, 2],\n",
       "       [1, 3],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_res=dist_matrix.map_blocks(map_blocks_1d_sparse,  dtype='int')\n",
    "da_res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T12:20:29.523893",
     "start_time": "2017-01-22T12:20:29.516823"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdist(points_np, points_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDAnalysis Data\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T10:02:23.148891",
     "start_time": "2017-01-22T09:59:58.261053"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHUNKSIZE=[128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "\n",
    "for c in CHUNKSIZE:\n",
    "    for i in FILENAMES:\n",
    "        print i\n",
    "        atoms = np.load(i)\n",
    "        a_da = da.from_array(atoms, chunks=(c,3))\n",
    "        out_file=os.path.join(OUT_DIR, os.path.basename(i)+\"_\"+str(c))\n",
    "        try:\n",
    "            os.makedirs(out_file)\n",
    "        except:\n",
    "            pass\n",
    "        da.to_npy_stack(out_file, a_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Output Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T19:02:00.812666",
     "start_time": "2017-01-21T19:02:00.767300"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "distances = dist_matrix.map_blocks(map_block_distance).compute()\n",
    "#da.to_npy_stack(output_directory, distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Output\n",
    "\n",
    "Outputs sparse edge list\n",
    "\n",
    "**Single Node**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T19:12:17.308631",
     "start_time": "2017-01-21T19:11:33.279674"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "CHUNKSIZE=1024\n",
    "FILENAME=\"atom_pos_291K.npy\"\n",
    "fn=os.path.join(OUT_DIR,'atom_pos_132K.npy_%s')%CHUNKSIZE\n",
    "point_array=da.from_npy_stack(fn)\n",
    "point_array.shape\n",
    "dist_matrix = da.zeros((point_array.shape[0],point_array.shape[0]), chunks=(CHUNKSIZE,point_array.shape[0]))\n",
    "da_res=dist_matrix.map_blocks(map_blocks_1d_sparse, chunks=(CHUNKSIZE,3), dtype='int')\n",
    "res=da_res.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T19:27:49.049930",
     "start_time": "2017-01-21T19:27:49.045408"
    }
   },
   "source": [
    "**Distributed**\n",
    "\n",
    "Data needs to be available on all nodes: \n",
    "    \n",
    "    rsync -avh comet-22-44:/scratch/luckow/7216144/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T12:11:29.474719",
     "start_time": "2017-01-22T12:11:29.421899"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from distributed import Client\n",
    "\n",
    "CHUNKSIZE=2048\n",
    "FILENAME=\"atom_pos_839K\"\n",
    "hostname=\"198.202.117.67:8786\"\n",
    "\n",
    "client = Client(hostname)\n",
    "cluster_details=client.ncores()\n",
    "number_nodes = len(cluster_details.keys())\n",
    "number_cores = sum(cluster_details.values())  \n",
    "    \n",
    "print \"Connected to Dask Cluster: %s \"%str(client.ncores())\n",
    "global point_array\n",
    "\n",
    "start=time.time()\n",
    "with dask.set_options(get=client.get):    \n",
    "    fn=os.path.join(OUT_DIR,'%s.npy_%s')%(FILENAME, CHUNKSIZE)\n",
    "    point_array=da.from_npy_stack(fn)\n",
    "    chunk_size = point_array.chunks[0][0]\n",
    "    print(\"Input Data: %s, Chunksize: %d, Numblocks: %s\")%(str(point_array.shape), chunk_size, str(point_array.numblocks))\n",
    "    dist_matrix = da.zeros((point_array.shape[0],point_array.shape[0]), chunks=(chunk_size,point_array.shape[0]))\n",
    "    da_res=dist_matrix.map_blocks(map_blocks_1d_sparse, chunks=(CHUNKSIZE,3), dtype='int')\n",
    "    #res=da_res.compute()\n",
    "end_compute=time.time()    \n",
    "print(\"%s,dask-distributed,%s, %d, %d, comet, total, %.4f\"%(fn, \"benchmark_dask_map_block_1d_sparse_distributed\", number_nodes, number_cores, end_compute-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T08:44:56.482891",
     "start_time": "2017-01-22T08:44:56.479833"
    }
   },
   "source": [
    "### Single Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T12:13:21.967949",
     "start_time": "2017-01-22T12:13:21.928263"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "OUTPUT_DIRECTORY=\"/oasis/scratch/comet/luckow/temp_project/out\"\n",
    "\n",
    "def benchmark_dask_map_block_1d_sparse_single_node(filename, cutoff=15, number_threads=40, direct_output=True ):\n",
    "    global point_array\n",
    "    func_name = sys._getframe().f_code.co_name\n",
    "\n",
    "    results = []\n",
    "    cache = Chest(path=os.path.join(BASE_DIRECTORY, \"cache\"), available_memory=98e9)  \n",
    "    \n",
    "    start = time.time()\n",
    "    point_array=da.from_npy_stack(filename)\n",
    "    chunk_size = point_array.chunks[0][0]\n",
    "    #print str(point_array.shape)\n",
    "    end_read = time.time()\n",
    "    results.append(\"%s,dask,%s,read_file, %.4f\"%(filename, func_name, end_read-start))\n",
    "    \n",
    "    dist_matrix = da.zeros((point_array.shape[0],point_array.shape[0]), chunks=(chunk_size, point_array.shape[0]))\n",
    "    #with ProgressBar():\n",
    "    \n",
    "    \"\"\"map_block_distances operates on point_array \"\"\"\n",
    "    #out =  dist_matrix.map_blocks(map_block_distance)\n",
    "    da_res=dist_matrix.map_blocks(map_blocks_1d_sparse, chunks=(chunk_size,3), dtype='int')\n",
    "    #res=da_res.compute(cache=cache)\n",
    "    res=da_res.compute()\n",
    "    #outfile = os.path.join(OUTPUT_DIRECTORY, os.path.basename(filename) + \"_out.h5\")\n",
    "    #da_res.to_hdf5(outfile, \"/o\", compression='lzf')\n",
    "    end_compute = time.time()\n",
    "    results.append(\"%s,dask,%s, 1, 24, comet, compute, %.4f\"%(filename, func_name, end_compute-end_read))\n",
    "    results.append(\"%s,dask,%s, 1, 24, comet, total, %.4f\"%(filename, func_name, end_compute-start))    \n",
    "    \n",
    "    # Log performance data\n",
    "    #end_compute = -1\n",
    "    #end_out_write = -1\n",
    "    #outfile = os.path.join(OUTPUT_DIRECTORY, os.path.basename(filename) + \"_out.h5\")\n",
    "    #try: \n",
    "    #    os.makedirs(outfile) \n",
    "    #except: \n",
    "    #    pass\n",
    "    \n",
    "    #if direct_output:\n",
    "    #    #da.to_npy_stack(outfile, out)    \n",
    "    #    out.to_hdf5(outfile, \"/o\", compression='lzf')\n",
    "    #    end_compute = time.time()\n",
    "    #    results.append(\"%s,dask,%s,compute_write, %.4f\"%(filename, func_name, end_compute-end_read))\n",
    "    #    results.append(\"%s,dask,%s,total, %.4f\"%(filename, func_name, end_compute-start))    \n",
    "    #else:\n",
    "    #    out.compute()  \n",
    "    #    end_compute = time.time()\n",
    "    #    #print \"end compute\"\n",
    "    #    np.save(outfile, out)\n",
    "    #    end_out_write = time.time()            \n",
    "    #    results.append(\"%s,dask,%s,compute, %.4f\"%(filename, func_name, end_compute-end_read))\n",
    "    #    results.append(\"%s,dask,%s,write_file, %.4f\"%(filename, func_name, end_out_write-end_compute))\n",
    "    #    results.append(\"%s,dask,%s,total, %.4f\"%(filename, func_name, end_out_write-start))\n",
    "    \n",
    "    #os.remove(outfile)\n",
    "    print(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T08:46:59.822086",
     "start_time": "2017-01-22T08:46:21.081873"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmark_dask_map_block_1d_sparse_single_node(os.path.join(OUT_DIR, \"atom_pos_132K.npy_512\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using too small parititions, the file open limit of the machine can easily be exhausted\n",
    "\n",
    "    ulimit -n 70000\n",
    "    \n",
    "Unfortunately this limit can not be increased on Comet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T08:46:59.825418",
     "start_time": "2017-01-22T08:46:59.823520"
    }
   },
   "source": [
    "### Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T16:32:30.374894",
     "start_time": "2017-01-22T16:32:30.343425"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from distributed import Client\n",
    "\n",
    "hostname=\"198.202.116.245:8786\"\n",
    "client = Client(hostname)\n",
    "    \n",
    "def benchmark_dask_map_block_1d_sparse_distributed(filename, cutoff=15, direct_output=True ):\n",
    "    global point_array\n",
    "    func_name = sys._getframe().f_code.co_name\n",
    "    results = []\n",
    "    #cache = Chest(path=os.path.join(BASE_DIRECTORY, \"cache\"), available_memory=98e9)  \n",
    "\n",
    "    cluster_details=client.ncores()\n",
    "    number_nodes = len(cluster_details.keys())\n",
    "    number_cores = sum(cluster_details.values())  \n",
    "    global point_array\n",
    "    with dask.set_options(get=client.get):        \n",
    "        start = time.time()\n",
    "        point_array=da.from_npy_stack(filename)\n",
    "        chunk_size = point_array.chunks[0][0]   \n",
    "        end_read = time.time()\n",
    "        results.append(\"%s,dask-distributed, %s, %d, %d, comet, read_file, %.4f\"%(filename, func_name, number_nodes, number_cores, end_read-start))\n",
    "        chunk_size\n",
    "        dist_matrix = da.zeros((point_array.shape[0],point_array.shape[0]), chunks=(chunk_size, point_array.shape[0]))\n",
    "        \"\"\"map_block_distances operates on point_array \"\"\"\n",
    "        da_res=dist_matrix.map_blocks(map_blocks_1d_sparse, chunks=(chunk_size,3), dtype='int')\n",
    "        res=da_res.compute()\n",
    "        end_compute = time.time()\n",
    "        results.append(\"%s,dask-distributed, %s, %d, %d, comet, compute, %.4f\"%(filename, func_name, number_nodes, number_cores, end_compute-end_read))\n",
    "        results.append(\"%s,dask-distributed, %s, %d, %d, comet, total, %.4f\"%(filename, func_name, number_nodes, number_cores, end_compute-start))    \n",
    "        print(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T12:48:09.444525",
     "start_time": "2017-01-22T12:47:37.806972"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmark_dask_map_block_1d_sparse_distributed(os.path.join(OUT_DIR, \"atom_pos_132K.npy_1024\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T09:49:50.460521",
     "start_time": "2017-01-22T09:49:30.091568"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#benchmark_dask_map_block_1d_sparse_distributed(os.path.join(OUT_DIR, \"atom_pos_291K.npy_2048\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T12:16:42.144177",
     "start_time": "2017-01-22T12:13:50.022922"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmark_dask_map_block_1d_sparse_distributed(os.path.join(OUT_DIR, \"atom_pos_839K.npy_2048\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T18:16:53.890247",
     "start_time": "2017-01-22T18:16:53.873182"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_839K.npy_16384',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_132K.npy_1024',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_132K.npy_16384',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_839K.npy_2048',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_145K.npy_1024',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_291K.npy_16384',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_839K.npy_1024',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_145K.npy_4096',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_132K.npy_4096',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_145K.npy_512',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_839K.npy_8192',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_132K.npy_512',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_291K.npy_1024',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_291K.npy_2048',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_145K.npy_2048',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_291K.npy_512',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_839K.npy_512',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_145K.npy_8192',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_291K.npy_4096',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_145K.npy_16384',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_839K.npy_4096',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_132K.npy_8192',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_132K.npy_2048',\n",
       " '/oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_291K.npy_8192']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_scenarios = [os.path.abspath(os.path.join(OUT_DIR, i)) for i in os.listdir(OUT_DIR)]\n",
    "shuffle(dask_scenarios)\n",
    "dask_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-23T02:17:04.110Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: /oasis/scratch/comet/luckow/temp_project/npy_stack/atom_pos_839K.npy_16384 (0/24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils - ERROR - (\"('map_blocks_1d_sparse-3151727e8eb0eb593212f632a9f1c414', 34, 0)\", '198.202.116.250:44898')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/luckow/anaconda2/lib/python2.7/site-packages/distributed/utils.py\", line 149, in f\n",
      "    result[0] = yield gen.maybe_future(func(*args, **kwargs))\n",
      "  File \"/home/luckow/anaconda2/lib/python2.7/site-packages/tornado/gen.py\", line 1015, in run\n",
      "    value = future.result()\n",
      "  File \"/home/luckow/anaconda2/lib/python2.7/site-packages/tornado/concurrent.py\", line 237, in result\n",
      "    raise_exc_info(self._exc_info)\n",
      "  File \"/home/luckow/anaconda2/lib/python2.7/site-packages/tornado/gen.py\", line 1021, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/home/luckow/anaconda2/lib/python2.7/site-packages/distributed/client.py\", line 896, in _gather\n",
      "    st.traceback)\n",
      "  File \"<string>\", line 2, in reraise\n",
      "KilledWorker: (\"('map_blocks_1d_sparse-3151727e8eb0eb593212f632a9f1c414', 34, 0)\", '198.202.116.250:44898')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed! Exception\n"
     ]
    }
   ],
   "source": [
    "for idx, s in enumerate(dask_scenarios):\n",
    "    #if '839K' in s:\n",
    "    print \"Process: %s (%d/%d)\"%(s, idx+1, len(dask_scenarios))\n",
    "    try:\n",
    "        benchmark_dask_map_block_1d_sparse_distributed(s)\n",
    "    except:\n",
    "        print \"Failed! Exception\"\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Optimization Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T19:17:44.191625",
     "start_time": "2017-01-21T19:17:44.173620"
    },
    "collapsed": false,
    "init_cell": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"Make sure point_array points to correct dataset\"\"\"\n",
    "global point_array\n",
    "global cutoff\n",
    "\n",
    "cutoff=15.0\n",
    "\n",
    "\n",
    "def map_block_distance_sparse(block,  block_id=None):\n",
    "    isCompute = block_id[1]>=block_id[0] and block.shape != (1,1)# Bug ? \n",
    "                                         # Dask returns one block with shape (1,1) ID:(1, 1) Shape: (1, 1) Predicate: True Content: [[ 1.]]\n",
    "    block_length = block.shape[0]\n",
    "    logging.debug(\"ID:\" + str(block_id) + \" Shape: \" + str(block.shape) + \\\n",
    "          \" Predicate: \" + str(isCompute) + \" Content: \" + str(block) + \"\\n\")\n",
    "    if isCompute:\n",
    "        source_start = block_id[0]*block_length\n",
    "        source_end = (block_id[0]+1)*block_length\n",
    "        source_points = point_array[source_start:source_end]        \n",
    "        dest_start = block_id[1]*block_length\n",
    "        dest_end = (block_id[1]+1)*block_length\n",
    "        dest_points = point_array[dest_start:dest_end]  \n",
    "        logging.debug(\"Source Idx: %d - %d Dest. Idx: %d - %d\\n\"%(source_start, source_end, dest_start, dest_end))\n",
    "        #print \"Source Points: \" + str(source_points.compute())\n",
    "        #print \"Destination Points: \" + str(dest_points.compute())\n",
    "        #return cdist(source_points, dest_points)<cutoff\n",
    "        #return np.array(block)\n",
    "        #else:\n",
    "    return np.array([zip(x,y) for x,y in zip(np.zeros(shape), np.ones(shape))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T19:58:06.521764",
     "start_time": "2017-01-20T19:58:06.518762"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_da = da.from_array(distances, chunks=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T19:58:07.222712",
     "start_time": "2017-01-20T19:58:07.183890"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances=dist_matrix.map_blocks(map_block_distance_sparse).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T19:50:42.662841",
     "start_time": "2017-01-20T19:50:42.659344"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape=(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T19:51:32.604487",
     "start_time": "2017-01-20T19:51:32.598158"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.dstack((np.zeros(shape), np.ones(shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T19:52:48.792012",
     "start_time": "2017-01-20T19:52:48.784087"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[zip(x,y) for x,y in zip(np.zeros(shape), np.ones(shape))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T19:55:33.475164",
     "start_time": "2017-01-20T19:55:33.469364"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T19:58:07.222712",
     "start_time": "2017-01-20T19:58:07.183890"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances=dist_matrix.map_blocks(map_block_distance).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T06:52:58.578700",
     "start_time": "2017-01-21T06:52:58.572961"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def atop_func(x, block_id):\n",
    "    print str(block_id)\n",
    "    print type(x)\n",
    "    print str(x)\n",
    "    for i in x:\n",
    "        return np.where(x)\n",
    "    print x[0].shape\n",
    "    if x==True: return 5\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T06:52:59.607793",
     "start_time": "2017-01-21T06:52:59.404119"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = da.atop(atop_func, 'i', dist_da, 'ij', dtype='int').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T18:54:14.912997",
     "start_time": "2017-01-20T18:54:14.903089"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "da.atop(lambda x: 2 if random.randint(0,1)==0 else 1, 'i', dist_matrix, 'ij', dtype='int').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T14:32:42.874689",
     "start_time": "2017-01-22T14:32:42.852198"
    },
    "collapsed": false,
    "init_cell": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Make sure point_array points to correct dataset\"\"\"\n",
    "global point_array\n",
    "global cutoff\n",
    "\n",
    "cutoff=15.0\n",
    "\n",
    "def map_blocks_new_axis_2d(block, block_id):\n",
    "    new_block = block[:, :, None]\n",
    "    \n",
    "    isCompute = block_id[1]>=block_id[0] and block.shape != (1,1)# Bug ? \n",
    "                                         # Dask returns one block with shape (1,1) ID:(1, 1) Shape: (1, 1) Predicate: True Content: [[ 1.]]\n",
    "    block_length = block.shape[0]\n",
    "    logging.debug(\"ID:\" + str(block_id) + \" Shape: \" + str(block.shape) + \\\n",
    "          \" Predicate: \" + str(isCompute) + \" Content: \" + str(block) + \"\\n\")\n",
    "    \n",
    "    source_start = block_id[0]*block_length\n",
    "    source_end = (block_id[0]+1)*block_length\n",
    "    source_points = point_array[source_start:source_end]        \n",
    "    dest_start = block_id[1]*block_length\n",
    "    dest_end = (block_id[1]+1)*block_length\n",
    "    dest_points = point_array[dest_start:dest_end]  \n",
    "    logging.debug(\"Source Idx: %d - %d Dest. Idx: %d - %d\\n\"%(source_start, source_end, dest_start, dest_end))\n",
    "    logging.debug(\"Source Points: \" + str(source_points.compute()))\n",
    "    logging.debug(\"Destination Points: \" + str(dest_points.compute()))\n",
    "    distances = cdist(source_points, dest_points) \n",
    "    distances_bool = (distances < cutoff) & (distances > 0)\n",
    "    xx, yy = np.meshgrid(np.arange(source_start, source_end), np.arange(dest_start, dest_end))\n",
    "    logging.debug(\"xx: \" + str(xx))\n",
    "    logging.debug(\"yy: \" + str(yy))\n",
    "    res=np.array([zip(x,y, z) for x,y,z in zip(yy, xx, distances_bool)])\n",
    "    logging.debug(\"Result: \" + str(res))\n",
    "    return res\n",
    "\n",
    "    #return new_block + cdist(source_points, dest_points)<cutoff\n",
    "    #return np.array(block)\n",
    "    #return new_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T18:53:54.073909",
     "start_time": "2017-01-21T18:53:54.065379"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d=(cdist(points_np,points_np)<cutoff)& (cdist(points_np,points_np)>0.0)\n",
    "print d\n",
    "zip(np.where(d)[0], np.where(d)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T18:36:31.379002",
     "start_time": "2017-01-21T18:36:31.374839"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(d)[0]+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T18:33:26.883438",
     "start_time": "2017-01-21T18:33:26.859124"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_res=df.from_array(da_res, columns=[\"From\", \"To\", \"isConnected\"])\n",
    "df_res.where(df_res[\"isConnected\"])\\\n",
    "      .dropna()[[\"From\", \"To\"]].values.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T18:17:08.938452",
     "start_time": "2017-01-21T18:17:08.933215"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Edges\n",
    "print str(distances)\n",
    "d=np.array(zip(np.where(distances)[0], np.where(distances)[1]))\n",
    "print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T18:24:52.045592",
     "start_time": "2017-01-21T18:24:52.040789"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip(np.where(distances)[0], np.where(distances)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T18:17:48.489797",
     "start_time": "2017-01-21T18:17:48.485329"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d[d[:,0]<d[:,1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-D with Sparse Output (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T07:18:58.026854",
     "start_time": "2017-01-22T07:18:58.021394"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_points = 4\n",
    "points_np = np.arange(number_points*3).reshape(number_points,3)\n",
    "point_array = da.from_array(points_np, chunks=(2, 3))\n",
    "dist_matrix = da.zeros((number_points,number_points), chunks=(2, 2)) # 2-D Chunks\n",
    "print dist_matrix.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T07:18:59.186237",
     "start_time": "2017-01-22T07:18:59.151459"
    },
    "collapsed": false,
    "init_cell": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Make sure point_array points to correct dataset\"\"\"\n",
    "global point_array\n",
    "global cutoff\n",
    "\n",
    "cutoff=15.0\n",
    "\n",
    "def map_blocks_new_axis_2d(block, block_id):\n",
    "    new_block = block[:, :, None]\n",
    "    \n",
    "    isCompute = block_id[1]>=block_id[0] and block.shape != (1,1)# Bug ? \n",
    "                                         # Dask returns one block with shape (1,1) ID:(1, 1) Shape: (1, 1) Predicate: True Content: [[ 1.]]\n",
    "    block_length = block.shape[0]\n",
    "    logging.debug(\"ID:\" + str(block_id) + \" Shape: \" + str(block.shape) + \\\n",
    "          \" Predicate: \" + str(isCompute) + \" Content: \" + str(block) + \"\\n\")\n",
    "    \n",
    "    source_start = block_id[0]*block_length\n",
    "    source_end = (block_id[0]+1)*block_length\n",
    "    source_points = point_array[source_start:source_end]        \n",
    "    dest_start = block_id[1]*block_length\n",
    "    dest_end = (block_id[1]+1)*block_length\n",
    "   \n",
    "    dest_points = point_array[dest_start:dest_end]  \n",
    "    logging.debug(\"Source Idx: %d - %d Dest. Idx: %d - %d\\n\"%(source_start, source_end, dest_start, dest_end))\n",
    "    logging.debug(\"Source Points: \" + str(source_points.compute()))\n",
    "    logging.debug(\"Destination Points: \" + str(dest_points.compute()))\n",
    "    distances = cdist(source_points, dest_points) \n",
    "    distances_bool = (distances < cutoff) & (distances > 0)\n",
    "    \n",
    "    logging.debug(str(distances_bool))\n",
    "    xx, yy = np.meshgrid(np.arange(source_start, source_end), np.arange(dest_start, dest_end))\n",
    "    logging.debug(\"xx: \" + str(xx))\n",
    "    logging.debug(\"yy: \" + str(yy))\n",
    "    #res=np.array([zip(y,x, z) for x,y,z in zip(xx, yy, distances_bool.T)])\n",
    "    res=np.array([zip(x,y, z) for x,y,z in zip(yy, xx, distances_bool)])\n",
    "    number_pairs = block.shape[0]*block.shape[1]\n",
    "    logging.debug(\"Result Shape: %s Block Shape: %s, Number Pairs: %d\"%(str(res.shape), str(block.shape), number_pairs))\n",
    "    res=res.reshape(number_pairs, 3)\n",
    "    logging.debug(\"Result: \" + str(res))\n",
    "    return res\n",
    "\n",
    "    #return new_block + cdist(source_points, dest_points)<cutoff\n",
    "    #return np.array(block)\n",
    "    #return new_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T07:19:05.897161",
     "start_time": "2017-01-22T07:19:05.892574"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "da_res=dist_matrix.map_blocks(map_blocks_new_axis_2d, chunks=(4,3), dtype='int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T07:19:11.895888",
     "start_time": "2017-01-22T07:19:11.860406"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "da_res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T15:46:28.865944",
     "start_time": "2017-01-21T15:46:28.484036"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_res=df.from_array(da_res, columns=[\"From\", \"To\", \"Distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T18:59:08.166479",
     "start_time": "2017-01-21T18:59:01.643542"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print \"Run with chunk size %d\"%CHUNKSIZE\n",
    "cache = Chest(path=os.path.join(BASE_DIRECTORY, \"cache\"), available_memory=98e9)  \n",
    "df_res=df.from_array(da_res, columns=[\"From\", \"To\", \"isConnected\"])\n",
    "edges_np=df_res.where(df_res[\"isConnected\"])\\\n",
    "               .dropna()[[\"From\", \"To\"]].values.compute(num_workers=12, cache=cache) #.to_hdf5(outfile, \"/o\", compression='lzf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T07:28:06.165515",
     "start_time": "2017-01-22T07:28:06.159164"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_points = 4\n",
    "points_np = np.arange(number_points*3).reshape(number_points,3)\n",
    "point_array = da.from_array(points_np, chunks=(2, 3))\n",
    "dist_matrix = da.zeros((number_points,number_points), chunks=(2,2)) # multiple chunks in 2. dimension\n",
    "print \"Dist Matrix Shape: \" + str(dist_matrix.shape)\n",
    "print \"Dist Matrix Chunks: \" + str(dist_matrix.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T14:32:42.909381",
     "start_time": "2017-01-22T14:32:42.875966"
    },
    "collapsed": false,
    "init_cell": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Make sure point_array points to correct dataset\"\"\"\n",
    "global point_array\n",
    "global cutoff\n",
    "\n",
    "cutoff=15.0\n",
    "\n",
    "def map_blocks_new_axis_2d(block, block_id):\n",
    "    new_block = block[:, :, None]\n",
    "    \n",
    "    isCompute = block_id[1]>=block_id[0] and block.shape != (1,1)# Bug ? \n",
    "                                         # Dask returns one block with shape (1,1) ID:(1, 1) Shape: (1, 1) Predicate: True Content: [[ 1.]]\n",
    "    block_length = block.shape[0]\n",
    "    logging.debug(\"ID:\" + str(block_id) + \" Shape: \" + str(block.shape) + \\\n",
    "          \" Predicate: \" + str(isCompute) + \" Content: \" + str(block) + \"\\n\")\n",
    "    \n",
    "    source_start = block_id[0]*block_length\n",
    "    source_end = (block_id[0]+1)*block_length\n",
    "    source_points = point_array[source_start:source_end]        \n",
    "    dest_start = block_id[1]*block_length\n",
    "    dest_end = (block_id[1]+1)*block_length\n",
    "   \n",
    "    dest_points = point_array[dest_start:dest_end]  \n",
    "    logging.debug(\"Source Idx: %d - %d Dest. Idx: %d - %d\\n\"%(source_start, source_end, dest_start, dest_end))\n",
    "    logging.debug(\"Source Points: \" + str(source_points.compute()))\n",
    "    logging.debug(\"Destination Points: \" + str(dest_points.compute()))\n",
    "    distances = cdist(source_points, dest_points) \n",
    "    distances_bool = (distances < cutoff) & (distances > 0)\n",
    "    # Get indicies with right offset from distnace bool\n",
    "    true_res = np.array(np.where(distances_bool))\n",
    "    logging.debug(\"True Source: %s, Source_start: %d\"%(str(true_res[0]), source_start))\n",
    "    true_res[0] = true_res[0] + source_start # source offset for block\n",
    "    logging.debug(\"True Source Adjusted\" + str(true_res[0]))\n",
    "    true_res[1] = true_res[1] + dest_start # dest offset for block\n",
    "    res=np.array(zip(true_res[0], true_res[1]))\n",
    "    res=res[res[:,0]<res[:,1], :] # filter duplicate ed\n",
    "    \n",
    "    number_pairs = len(res)\n",
    "    logging.debug(\"Result Shape: %s Block Shape: %s, Number Pairs: %d\"%(str(res.shape), str(block.shape), number_pairs))\n",
    "    res=res.reshape(number_pairs, 2)\n",
    "    logging.debug(\"Result: \" + str(res))\n",
    "    return new_block + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T07:31:13.207908",
     "start_time": "2017-01-22T07:31:13.109516"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_res=dist_matrix.map_blocks(map_blocks_new_axis_2d, dtype='int').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T07:27:51.266937",
     "start_time": "2017-01-22T07:27:51.262847"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "da_res"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Initialisation Cell",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "68px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "1568px",
    "left": "0px",
    "right": "1355px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
