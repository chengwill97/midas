{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, ast, time, pickle, redis\n",
    "import numpy as np\n",
    "from pykafka import KafkaClient\n",
    "from pykafka.partitioners import hashing_partitioner\n",
    "from scipy.spatial import distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_kafka(window):\n",
    "\n",
    "    start_consumption = time.time()\n",
    "    Nmessages = 0\n",
    "    data = []\n",
    "    while time.time() - start_consumption < window:\n",
    "        message = consumer.consume(block=True) \n",
    "        if message!=None:\n",
    "            message = message.value\n",
    "            data_np = np.array(ast.literal_eval(message))\n",
    "            data.append(data_np)\n",
    "            Nmessages+=1  # save that\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters():\n",
    "\n",
    "    serialized_clusters =  r.get('means')\n",
    "    return pickle.loads(serialized_clusters)\n",
    "\n",
    "\n",
    "def save_sums_to_redis(partial_sums):\n",
    "    r.rpush('partial_sums',pickle.dumps(partial_sums))\n",
    "    return\n",
    "\n",
    "def calculate_distances(elements,centroids):      # np.array of 3-d data\n",
    "\n",
    "    return distance.cdist(elements, centroids, 'euclidean') # row: elements , column :centroids \n",
    "\n",
    "\n",
    "def find_partial_sums(dist, centroids,elements):\n",
    "\n",
    "    #ncentroids = centroids.shape[0]\n",
    "    dtype = str(centroids.shape) + 'float64,(' + str(centroids.shape[0]) + ',1)float32'\n",
    "    sum_centroids =  np.zeros(1, dtype=dtype)    # first column is the sum of centroids 2nd is the number of elements\n",
    "                                                 #access sum of centroids [0][0]\n",
    "                                                 # acess sum of elements: [0][1]\n",
    "    centroid_pos = np.argmin(dist, axis=1)  #  index: element id - value:  closest centroid_id\n",
    "\n",
    "    ## sum all distances of each cluster \n",
    "    for i in  xrange(len(elements)):\n",
    "        centroid = centroid_pos[i]\n",
    "        sum_centroids[0][0][centroid] += elements[i]  # add also number of elements\n",
    "        sum_centroids[0][1][centroid] +=1  # added one element to that cluster\n",
    "\n",
    "    return  sum_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## settings\n",
    "\n",
    "zkKafka = 'localhost:2181'\n",
    "broker = 'localhost:9092'\n",
    "redis_host = 'localhost'\n",
    "kafka_messages = 10000\n",
    "window = 10000  # in miliseconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = KafkaClient(hosts=broker)\n",
    "topic = client.topics['Throughput']\n",
    "consumer = topic.get_simple_consumer(reset_offset_on_start=True,consumer_timeout_ms=10000)\n",
    "r = redis.StrictRedis(host=redis_host, port=6379, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.84 ms, sys: 1.47 ms, total: 5.3 ms\n",
      "Wall time: 4.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## main function\n",
    "#data_batch = get_data_from_kafka(window)\n",
    "\n",
    "#process\n",
    "centroids = get_clusters()\n",
    "elements = np.concatenate(data_batch,axis=0) # fix shape\n",
    "elements = np.concatenate(data_batch,axis=0).reshape(elements.size/3,3)[0]\n",
    "dist = calculate_distances(data_batch[0], centroids)\n",
    "partial_sums = find_partial_sums(dist, centroids, elements)\n",
    "save_sums_to_redis(partial_sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import redis\n",
    "\n",
    "\n",
    "\n",
    "def get_and_aggregate_partial_sums_from_redis(clusters):\n",
    "    \"\"\"\n",
    "    - Each worker is writing the partial sums to redis server and now I\n",
    "    - Aggregate the partial sums from all the workers, in order to calculate the new centroids\n",
    "    \"\"\"\n",
    "    aggregated_sums_of_elements = np.zeros(clusters.shape)  # agggregate sum for each \n",
    "                                                             #centroid - shape[1] is dim of element\n",
    "    n_elements_per_cluster = np.zeros((clusters.shape[0], 1))   # number of \n",
    "                                                            #elements that belongs to each centroid\n",
    "    entries = r.llen('partial_sums')    # number of entries from workers\n",
    "\n",
    "    for i in xrange(entries):\n",
    "        serialized_value = r.lindex('partial_sums', i)\n",
    "        apartial_sum = pickle.loads(serialized_value)\n",
    "        n_elements_per_cluster = apartial_sum[0][0]   \n",
    "        aggregated_sums_of_elements += apartial_sum[0][1]  ## adds up the element to all correct cluster \n",
    "\n",
    "    # delete entries\n",
    "    r.delete('partial_sums')\n",
    "\n",
    "    return  (aggregated_sums_of_elements, n_elements_per_cluster)\n",
    "\n",
    "\n",
    "\n",
    "def find_new_centers(data):\n",
    "    \"\"\"\n",
    "    Add docstring\n",
    "    \"\"\"\n",
    "    return   np.divide(data[0], data[1]) \n",
    "\n",
    "\n",
    "def save_clusters_to_redis(clusters):\n",
    "    \"\"\"\n",
    "    - Add docstring\n",
    "    \"\"\"\n",
    "    r.set('means', pickle.dumps(clusters))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def get_clusters():\n",
    "\n",
    "    serialized_clusters =  r.get('means')\n",
    "\n",
    "    return pickle.loads(serialized_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = get_clusters()\n",
    "par_sums = get_and_aggregate_partial_sums_from_redis(centroids)\n",
    "centroids = np.nan_to_num(find_new_centers(par_sums))\n",
    "save_clusters_to_redis(centroids)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
